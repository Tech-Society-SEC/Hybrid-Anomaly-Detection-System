{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87d2a77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pykalman\n",
      "  Downloading pykalman-0.10.2-py2.py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from pykalman) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pykalman) (25.0)\n",
      "Collecting scikit-base<0.13.0 (from pykalman)\n",
      "  Downloading scikit_base-0.12.6-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: scipy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from pykalman) (1.15.3)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3->pykalman) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3->pykalman) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3->pykalman) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3->pykalman) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3->pykalman) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3->pykalman) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3->pykalman) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3->pykalman) (2022.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3->pykalman) (2024.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3->pykalman) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3->pykalman) (2024.2.0)\n",
      "Downloading pykalman-0.10.2-py2.py3-none-any.whl (249 kB)\n",
      "Downloading scikit_base-0.12.6-py3-none-any.whl (149 kB)\n",
      "Installing collected packages: scikit-base, pykalman\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [pykalman]\n",
      "\u001b[1A\u001b[2KSuccessfully installed pykalman-0.10.2 scikit-base-0.12.6\n"
     ]
    }
   ],
   "source": [
    "!pip install pykalman\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aade2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Import statsmodels for ARIMA\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Import pykalman for Kalman Filter\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "# Import utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3836075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the column names based on the readme.txt\n",
    "col_names = ['unit_number', 'time_in_cycles', 'setting_1', 'setting_2', 'setting_3']\n",
    "col_names.extend([f'sensor_{i}' for i in range(1, 22)])\n",
    "\n",
    "# Load the training data\n",
    "df_train = pd.read_csv('/CMAPSSData/train_FD001.txt', sep=' ', header=None, names=col_names)\n",
    "\n",
    "# Drop the last two columns which are empty (NaNs)\n",
    "df_train = df_train.drop(columns=[26, 27])\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Data loaded successfully:\")\n",
    "display(df_train.head())\n",
    "\n",
    "# Display data info\n",
    "print(\"\\nData Info:\")\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b19c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate standard deviation for each feature\n",
    "data_std = df_train.std()\n",
    "print(\"Standard Deviations:\")\n",
    "print(data_std)\n",
    "\n",
    "# Identify columns with zero (or very low) variance\n",
    "# These columns provide no information for the FD001 dataset\n",
    "cols_to_drop = data_std[data_std < 1e-5].index.tolist()\n",
    "print(f\"\\nConstant columns to drop: {cols_to_drop}\")\n",
    "\n",
    "# Drop these columns from the dataframe\n",
    "df_train_processed = df_train.drop(columns=cols_to_drop)\n",
    "\n",
    "# Display the shape of the new dataframe\n",
    "print(f\"\\nOriginal shape: {df_train.shape}\")\n",
    "print(f\"Processed shape: {df_train_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75a869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of remaining sensor columns\n",
    "sensor_cols = [col for col in df_train_processed.columns if 'sensor' in col]\n",
    "print(f\"Remaining sensors to scale: {sensor_cols}\")\n",
    "\n",
    "# Initialize the MinMax scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Create a copy to hold the scaled data\n",
    "df_train_scaled = df_train_processed.copy()\n",
    "\n",
    "# Fit and transform the sensor data\n",
    "df_train_scaled[sensor_cols] = scaler.fit_transform(df_train_scaled[sensor_cols])\n",
    "\n",
    "print(\"\\nScaled Data Head:\")\n",
    "display(df_train_scaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ccad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all data for unit 1\n",
    "engine_1_data = df_train_scaled[df_train_scaled['unit_number'] == 1].copy()\n",
    "\n",
    "# Select sensor_7 as our time series\n",
    "ts = engine_1_data['sensor_7'].reset_index(drop=True)\n",
    "\n",
    "# Plot the chosen sensor data\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(ts, label='Sensor 7 - Unit 1')\n",
    "plt.title('Full Time Series (Sensor 7, Unit 1)')\n",
    "plt.xlabel('Time in Cycles')\n",
    "plt.ylabel('Normalized Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba067119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the split point\n",
    "split_point = 50 \n",
    "\n",
    "# Create training and testing sets\n",
    "train_data = ts.iloc[:split_point]\n",
    "test_data = ts.iloc[split_point:]\n",
    "\n",
    "# Plot the split\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(train_data, label='Train (Healthy)')\n",
    "plt.plot(test_data, label='Test (Degrading)')\n",
    "plt.title('Train/Test Split')\n",
    "plt.xlabel('Time in Cycles')\n",
    "plt.axvline(split_point, color='red', linestyle='--', label='Split')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efba501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Find 'd' (Order of Differencing) ---\n",
    "print(\"--- Finding 'd' (Differencing) ---\")\n",
    "adf_result = adfuller(train_data)\n",
    "print(f'ADF Statistic: {adf_result[0]}')\n",
    "print(f'p-value: {adf_result[1]}')\n",
    "\n",
    "# If p-value is > 0.05, we need to difference\n",
    "if adf_result[1] > 0.05:\n",
    "    print(\"Data is non-stationary. Differencing once (d=1).\")\n",
    "    d = 1\n",
    "    # Check p-value after differencing\n",
    "    adf_result_diff = adfuller(train_data.diff().dropna())\n",
    "    print(f'p-value after 1st diff: {adf_result_diff[1]}')\n",
    "else:\n",
    "    print(\"Data is stationary (d=0).\")\n",
    "    d = 0\n",
    "\n",
    "# --- 2. Find 'p' and 'q' ---\n",
    "print(\"\\n--- Finding 'p' and 'q' ---\")\n",
    "# Plot ACF and PACF on the (now stationary) data\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "plot_acf(train_data.diff().dropna(), lags=20, ax=ax1)\n",
    "plot_pacf(train_data.diff().dropna(), lags=20, ax=ax2, method='ywm')\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "Based on the plots (example interpretation):\n",
    "- PACF (top): Cuts off sharply after lag 1. Suggests p=1.\n",
    "- ACF (bottom): Tails off.\n",
    "Let's choose p=1, d=1, q=1 as a robust starting point.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91bbe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting ARIMA rolling forecast...\")\n",
    "\n",
    "history = list(train_data)\n",
    "predictions = []\n",
    "anomaly_scores_arima = []\n",
    "\n",
    "for t in range(len(test_data)):\n",
    "    # Create and fit the ARIMA model\n",
    "    model = ARIMA(history, order=(1, 1, 1))\n",
    "    model_fit = model.fit()\n",
    "    \n",
    "    # Forecast the next step\n",
    "    yhat = model_fit.forecast()[0]\n",
    "    predictions.append(yhat)\n",
    "    \n",
    "    # Get the true observation\n",
    "    obs = test_data.iloc[t]\n",
    "    \n",
    "    # Calculate the anomaly score (Absolute Error)\n",
    "    error = abs(obs - yhat)\n",
    "    anomaly_scores_arima.append(error)\n",
    "    \n",
    "    # Add the new observation to history\n",
    "    history.append(obs)\n",
    "\n",
    "print(\"ARIMA forecast complete.\")\n",
    "\n",
    "# Store results in a dataframe\n",
    "arima_results = pd.DataFrame({\n",
    "    'actual': test_data.values,\n",
    "    'predicted': predictions,\n",
    "    'anomaly_score': anomaly_scores_arima\n",
    "}, index=test_data.index)\n",
    "\n",
    "display(arima_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8ac989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Actual vs. Predicted\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(arima_results['actual'], label='Actual Value')\n",
    "plt.plot(arima_results['predicted'], label='ARIMA Prediction', linestyle='--', alpha=0.7)\n",
    "plt.title('ARIMA: Actual vs. Predicted')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Anomaly Scores\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(arima_results['anomaly_score'], label='ARIMA Anomaly Score', color='orange')\n",
    "plt.title('ARIMA Anomaly Scores (Absolute Error)')\n",
    "plt.xlabel('Time in Cycles')\n",
    "plt.ylabel('Anomaly Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27979087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the full time series `ts` for smoothing\n",
    "data_full = ts.values\n",
    "\n",
    "# Configure the Kalman Filter for a simple state model\n",
    "# (e.g., value = last_value + noise)\n",
    "kf = KalmanFilter(\n",
    "    transition_matrices = [1],         # State evolves by 1*last_state (A)\n",
    "    observation_matrices = [1],        # Observation is 1*current_state (H)\n",
    "    initial_state_mean = data_full[0], # Initial guess\n",
    "    initial_state_covariance = 1,      # Initial uncertainty\n",
    "    observation_covariance = 1,        # Measurement noise (R) - TUNE THIS\n",
    "    transition_covariance = 0.01       # Process noise (Q) - TUNE THIS\n",
    ")\n",
    "\n",
    "# Run the Kalman Filter smoother over all data\n",
    "(smoothed_state_means, smoothed_state_covariances) = kf.smooth(data_full)\n",
    "\n",
    "# Calculate anomaly score (residual)\n",
    "anomaly_scores_kf = np.abs(data_full - smoothed_state_means.flatten())\n",
    "\n",
    "# Store in a dataframe\n",
    "kf_results = pd.DataFrame({\n",
    "    'actual': data_full,\n",
    "    'smoothed': smoothed_state_means.flatten(),\n",
    "    'anomaly_score': anomaly_scores_kf\n",
    "}, index=ts.index)\n",
    "\n",
    "print(\"Kalman Filter smoothing complete.\")\n",
    "display(kf_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c364d474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Actual vs. Smoothed\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(kf_results['actual'], label='Actual Value (Noisy)', alpha=0.7)\n",
    "plt.plot(kf_results['smoothed'], label='Kalman Filter Estimate', linestyle='--', color='green')\n",
    "plt.title('Kalman Filter: Noisy vs. Smoothed Estimate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Anomaly Scores\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(kf_results['anomaly_score'], label='Kalman Filter Anomaly Score', color='green')\n",
    "plt.title('Kalman Filter Anomaly Scores (Residuals)')\n",
    "plt.xlabel('Time in Cycles')\n",
    "plt.ylabel('Anomaly Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cad80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a final results dataframe\n",
    "df_final_scores = pd.DataFrame({\n",
    "    'ARIMA_Score': arima_results['anomaly_score'],\n",
    "    'KF_Score': kf_results['anomaly_score']\n",
    "})\n",
    "\n",
    "# We only have ARIMA scores for the 'test' portion. Let's compare there.\n",
    "df_final_scores = df_final_scores.loc[split_point:]\n",
    "\n",
    "# Scale the scores from 0 to 1 for comparison\n",
    "scaler_scores = MinMaxScaler()\n",
    "df_final_scores_scaled = pd.DataFrame(\n",
    "    scaler_scores.fit_transform(df_final_scores),\n",
    "    columns=df_final_scores.columns,\n",
    "    index=df_final_scores.index\n",
    ")\n",
    "\n",
    "# Plot the comparison\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(df_final_scores_scaled['ARIMA_Score'], label='ARIMA Anomaly Score (Scaled)')\n",
    "plt.plot(df_final_scores_scaled['KF_Score'], label='Kalman Filter Anomaly Score (Scaled)')\n",
    "plt.title('Baseline Model Anomaly Score Comparison (FD001, Unit 1, Sensor 7)')\n",
    "plt.xlabel('Time in Cycles')\n",
    "plt.ylabel('Normalized Anomaly Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
